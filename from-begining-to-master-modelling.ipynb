{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.EDA\n*** \n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"12\"></a> \n# 1.1 IMPORT MODULES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV,LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.decomposition import PCA\nfrom catboost import CatBoostRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nfrom scipy.stats import  skew\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T21:11:33.595280Z","iopub.execute_input":"2023-04-25T21:11:33.595718Z","iopub.status.idle":"2023-04-25T21:11:33.603983Z","shell.execute_reply.started":"2023-04-25T21:11:33.595680Z","shell.execute_reply":"2023-04-25T21:11:33.602823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.115388Z","iopub.execute_input":"2023-04-25T20:19:27.115882Z","iopub.status.idle":"2023-04-25T20:19:27.127392Z","shell.execute_reply.started":"2023-04-25T20:19:27.115832Z","shell.execute_reply":"2023-04-25T20:19:27.126187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/regression-technique-eda/House Price Regression Tecnique/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/regression-technique-eda/House Price Regression Tecnique/test.csv\")\ndf = train.append(test,ignore_index=False).reset_index()\ndf = df.drop(\"index\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.129156Z","iopub.execute_input":"2023-04-25T20:19:27.129635Z","iopub.status.idle":"2023-04-25T20:19:27.219644Z","shell.execute_reply.started":"2023-04-25T20:19:27.129586Z","shell.execute_reply":"2023-04-25T20:19:27.218621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a> \n# 1.2 GENERAL INFO OF DATA SET","metadata":{}},{"cell_type":"code","source":"def dataframe_info(df):\n    print(\"Info\")\n    print(df.info())\n    print(\"#\"*40)\n    print(\"DESCRIPTION\")\n    print(df.describe())\n    print(\"*\"*40)\n    print(\"NAN VALUES\")\n    print(df.isna().sum())\n    print(\"*\"*40)\n    print(\"# OF UNIQUES\")\n    print(df.nunique())\n    print(\"*\"*40)\n    print(\"# OF DUPLICATED\")\n    print(df.duplicated().sum())\n    print(\"*\"*40)\n    print(\"UNIQUES\")\n    for i in df.columns:\n        if  df[i].nunique()>20:\n            print(\"Contains more than 20 unique value.\")\n            print(i,\":\",df[i].nunique(),df[i].unique()[:20],end=\"\\n\\n\\n\")\n        else:\n            print(\"Contains less than 20 unique value.\")\n            print(i,\":\",df[i].nunique(),df[i].unique()[:20],end=\"\\n\\n\\n\")\n\n        \ndataframe_info(df)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.222964Z","iopub.execute_input":"2023-04-25T20:19:27.223664Z","iopub.status.idle":"2023-04-25T20:19:27.455570Z","shell.execute_reply.started":"2023-04-25T20:19:27.223610Z","shell.execute_reply":"2023-04-25T20:19:27.454189Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this function,i will categorized numerical columns, categorical columns. The reason of this function is that some numerical values look like categorical. Let me expalin. In this data set, OverallQuall looks like numerical. If you check closely the data, each numebrs are categorical.\n\nOverallQuall:\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\n       5\tAverage\n       4\tBelow Average\n       3\tFair\n       2\tPoor\n       1\tVery Poor","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>Grab_col_names:</b>  It gives the names of categorical, numerical and categorical but cardinal variables in the data set.\n</div>\n","metadata":{}},{"cell_type":"code","source":"def grab_col_names(dataframe,cat_th=16,car_th=20):\n    \"\"\"\n    It gives the names of categorical, numerical and categorical but cardinal variables in the data set.\n    Note: Categorical variables with numerical appearance are also included in categorical variables.\n\n    Parameters\n    ------\n            df: Dataframe\n                The dataframe from which variable names are to be retrieved\n        cat_th: int, optional\n                threshold value for numeric but categorical variables\n        car_th: int, optinal\n                threshold value for categorical but cardinal variables\n\n    Returns\n    ------\n        cat_cols: list\n                Categorical variable list\n        num_cols: list\n                Numeric variable list\n        cat_but_car: list\n                Categorical but cardinal variable list\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = total number of variables\n        num_but_cat is inside cat_cols\n   \"\"\"\n     #Category & cardinal columns\n    cat_cols =[col for col in df.columns if df[col].dtypes==\"O\"]\n    num_but_cats = [col for col in df.columns if (df[col].nunique()<cat_th) and df[col].dtypes != \"O\"]\n    cat_but_car = [col for col in df.columns if (df[col].nunique()>car_th) and df[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cats\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    #Numerical columns\n    num_cols = [col for col in df.columns if df[col].dtypes !=\"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cats]\n\n    print(f\"Observations: {df.shape[0]}\")\n    print(f\"Variables: {df.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cats)}')\n    return cat_cols, num_cols, cat_but_car\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.459408Z","iopub.execute_input":"2023-04-25T20:19:27.459868Z","iopub.status.idle":"2023-04-25T20:19:27.470728Z","shell.execute_reply.started":"2023-04-25T20:19:27.459831Z","shell.execute_reply":"2023-04-25T20:19:27.469217Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.471957Z","iopub.execute_input":"2023-04-25T20:19:27.472324Z","iopub.status.idle":"2023-04-25T20:19:27.520333Z","shell.execute_reply.started":"2023-04-25T20:19:27.472288Z","shell.execute_reply":"2023-04-25T20:19:27.518918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. HANDLING OUTLIERS\n***","metadata":{}},{"cell_type":"markdown","source":"**Tresholds Control & Rearrange it.**","metadata":{}},{"cell_type":"code","source":"def outlier_tresholds(df,col_name,qu1=0.05,qu3=0.95):\n    q1 = df[col_name].quantile(qu1)\n    q3 = df[col_name].quantile(qu3)\n    iqr=q3-q1\n    lower = q1-1.5*iqr\n    upper = q3+1.5*iqr\n    return lower,upper\n\ndef check_outlier(df,col_name):\n    lower_bond, upper_bond = outlier_tresholds(df,col_name)\n\n    if df[(df[col_name]<lower_bond) | (df[col_name]>upper_bond)].any(axis=None):\n        return True\n    else:\n        return False\n\n\noutliers=[]\nfor i in num_cols:\n    if check_outlier(df,i):\n        outliers.append(i)\nprint(outliers)\n\ndef check_outlier_cat(df,col_name):\n        upper_0 = []\n        for i in col_name:\n            lower, upper = outlier_tresholds(df, i)\n            print(i)\n            print(df[df[i]>upper][\"SalePrice\"].nunique())\n            print(\"OUTLIER:\")\n            print(\"UPPER\",upper)\n            print(df[df[i]>upper][i])\n            print(df[i].head())\n            print(\"**\"*20)\n\n            if upper==0.0:\n                upper_0.append(i)\n        return upper_0\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_tresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.521876Z","iopub.execute_input":"2023-04-25T20:19:27.523132Z","iopub.status.idle":"2023-04-25T20:19:27.696105Z","shell.execute_reply.started":"2023-04-25T20:19:27.523078Z","shell.execute_reply":"2023-04-25T20:19:27.694894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in outliers:\n    replace_with_thresholds(df,i)\n\ncheck_outlier_cat(df,outliers)\n\n\nfor i in outliers:\n    print(i,check_outlier(df,i))\n\n#From now on, there is no treshold.\n\n# Work on Missing Values\nnull_columns = []\n\n\nfor i in df.columns:\n   if (df[i].isna()>0).any(axis=None):\n       null_columns.append(i)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.697816Z","iopub.execute_input":"2023-04-25T20:19:27.698361Z","iopub.status.idle":"2023-04-25T20:19:27.948398Z","shell.execute_reply.started":"2023-04-25T20:19:27.698308Z","shell.execute_reply":"2023-04-25T20:19:27.947244Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. MISSING VALUES\n***","metadata":{}},{"cell_type":"markdown","source":"# 3.1 CATEGRORIZE NULL VALUES","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\"> \n<b>BE CAREFUL:</b> According yo data_description, BsmtQual,BsmtCond, BsmtExposure,BsmtFinType1,BsmtFinType2,FireplaceQu,GarageType,GarageFinish, BsmtCond\nGarageQual,GarageCond,PoolQC,Fence,MiscFeature,GarageYrBlt,MasVnrType,MasVnrArea. NA values mean NO. For exmaple, BsmtCond: NA means No Basement. Also, some of them have direct realtion btw them.\nFor example, GarageType has na values. It means no garage. Also, Data contains GarageYrBlt. It has na values. Those na means no garage.Then, i will create another list for them and rearange null_columns.\n</div>\n","metadata":{}},{"cell_type":"code","source":"null_but_not = [\"BsmtQual\", \"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\", \"GarageQual\",\"GarageCond\",\"PoolQC\",\"Fence\",\n                \"MiscFeature\",\"Alley\",\"GarageYrBlt\",\"BsmtCond\",\"MasVnrType\",\"MasVnrArea\",\"SalePrice\",\"Neighborhood\"]\n\nnull_columns = [col for col in null_columns if col not in null_but_not]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.950043Z","iopub.execute_input":"2023-04-25T20:19:27.950400Z","iopub.status.idle":"2023-04-25T20:19:27.956146Z","shell.execute_reply.started":"2023-04-25T20:19:27.950367Z","shell.execute_reply":"2023-04-25T20:19:27.954802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before filling the NA values, i will check the relation btw na columns and target columns if NA value is random or not.**","metadata":{}},{"cell_type":"code","source":"def missing_vs_target(df,target,na_columns):\n    temp_df = df.copy()\n\n    for col in na_columns:\n        temp_df[col +\"_NA_FLAG\"] = np.where(temp_df[col].isnull(),1,0)\n\n    na_flags = temp_df.loc[:,temp_df.columns.str.contains(\"_NA_\")].columns\n\n    for col in na_flags:\n        print(pd.DataFrame({\"TARGET_MEAN\":temp_df.groupby(col)[target].mean(),\n                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.960585Z","iopub.execute_input":"2023-04-25T20:19:27.960962Z","iopub.status.idle":"2023-04-25T20:19:27.969287Z","shell.execute_reply.started":"2023-04-25T20:19:27.960927Z","shell.execute_reply":"2023-04-25T20:19:27.968082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_vs_target(df,\"SalePrice\",null_columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:27.971110Z","iopub.execute_input":"2023-04-25T20:19:27.971453Z","iopub.status.idle":"2023-04-25T20:19:28.048205Z","shell.execute_reply.started":"2023-04-25T20:19:27.971422Z","shell.execute_reply":"2023-04-25T20:19:28.047016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see null grade graph.","metadata":{}},{"cell_type":"code","source":"def na_columns_histogram(df,na_cols):\n    for col in na_cols:\n        df[col].hist()\n        plt.title(\"Variable names: {}, Variable type: {} and # of unique:{}\".format(col,df[col].dtypes,df[col].nunique()))\n        plt.xlabel(\"Total NA values: {}\".format(df[col].isna().sum()))\n        plt.grid(False)\n        plt.show(block=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:28.050210Z","iopub.execute_input":"2023-04-25T20:19:28.050570Z","iopub.status.idle":"2023-04-25T20:19:28.056809Z","shell.execute_reply.started":"2023-04-25T20:19:28.050517Z","shell.execute_reply":"2023-04-25T20:19:28.055829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"na_columns_histogram(df,null_columns)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:28.058631Z","iopub.execute_input":"2023-04-25T20:19:28.058996Z","iopub.status.idle":"2023-04-25T20:19:31.508945Z","shell.execute_reply.started":"2023-04-25T20:19:28.058960Z","shell.execute_reply":"2023-04-25T20:19:31.507628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2 FILL NA VALUES","metadata":{}},{"cell_type":"code","source":"num_null_cols = [col for col in null_columns if df[col].dtypes != \"O\"]\n\nfor i in num_null_cols:\n    df[i] = df[i].fillna(df[i].median())\n\nprint(\"numerical null features\",df[num_null_cols].isna().sum())\n\ncat_null_cols = [col for col in null_columns if df[col].dtypes ==\"O\"]\n\nfor i in cat_null_cols:\n    df[i] = df[i].fillna(df[i].mode()[0])\nprint(\"categorical null features\",df[cat_null_cols].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:31.510604Z","iopub.execute_input":"2023-04-25T20:19:31.511808Z","iopub.status.idle":"2023-04-25T20:19:31.543760Z","shell.execute_reply.started":"2023-04-25T20:19:31.511757Z","shell.execute_reply":"2023-04-25T20:19:31.542397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Rare Encoding\n***","metadata":{}},{"cell_type":"code","source":"\ndef rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() / len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n\n    return temp_df","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:31.545220Z","iopub.execute_input":"2023-04-25T20:19:31.548881Z","iopub.status.idle":"2023-04-25T20:19:31.556619Z","shell.execute_reply.started":"2023-04-25T20:19:31.548842Z","shell.execute_reply":"2023-04-25T20:19:31.555335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = rare_encoder(df,0.011)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:31.558134Z","iopub.execute_input":"2023-04-25T20:19:31.560850Z","iopub.status.idle":"2023-04-25T20:19:31.651855Z","shell.execute_reply.started":"2023-04-25T20:19:31.560806Z","shell.execute_reply":"2023-04-25T20:19:31.650761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. FEATURE SELECTION - 1\n***","metadata":{}},{"cell_type":"code","source":"df.isna().sum()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:31.653380Z","iopub.execute_input":"2023-04-25T20:19:31.653913Z","iopub.status.idle":"2023-04-25T20:19:31.673256Z","shell.execute_reply.started":"2023-04-25T20:19:31.653868Z","shell.execute_reply":"2023-04-25T20:19:31.672036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n<b>NOTE:</b> As i explained above, fill na values with No. Also, let's find out numerical and categorical columns\n</div>","metadata":{}},{"cell_type":"code","source":"na_num=[col for col in df.columns if df[col].dtypes != \"O\" and (df[col].isna().sum()>0).any(axis=None)]\nna_cat=[col for col in df.columns if df[col].dtypes == \"O\" and (df[col].isna().sum()>0).any(axis=None)]\n\n\ndf[na_cat]=df[na_cat].apply(lambda x: np.where(x.isna(),\"No\",x))","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:31.674797Z","iopub.execute_input":"2023-04-25T20:19:31.675724Z","iopub.status.idle":"2023-04-25T20:19:31.721957Z","shell.execute_reply.started":"2023-04-25T20:19:31.675667Z","shell.execute_reply":"2023-04-25T20:19:31.720761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_list = []\nfor i in df.columns:\n    temp=pd.DataFrame({i: df[i].value_counts(),\n                  \"Mean\": np.round(df.groupby(i)[\"SalePrice\"].mean(), 0),\n                  \"Ratio\": df[i].value_counts() / len(df[i])})\n    if (temp[temp.Ratio>0.95]).any(axis=None):\n        drop_list.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:31.723511Z","iopub.execute_input":"2023-04-25T20:19:31.724453Z","iopub.status.idle":"2023-04-25T20:19:32.089879Z","shell.execute_reply.started":"2023-04-25T20:19:31.724405Z","shell.execute_reply":"2023-04-25T20:19:32.088918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\"> \n<b>BE CAREFUL:</b> Neighborhood feature has high cardinality, thus it is added to drop list.\n</div>\n","metadata":{}},{"cell_type":"code","source":"drop_list.append(\"Neighborhood\")","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:32.091199Z","iopub.execute_input":"2023-04-25T20:19:32.092376Z","iopub.status.idle":"2023-04-25T20:19:32.097800Z","shell.execute_reply.started":"2023-04-25T20:19:32.092324Z","shell.execute_reply":"2023-04-25T20:19:32.096502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(drop_list,axis=1,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:32.099463Z","iopub.execute_input":"2023-04-25T20:19:32.100190Z","iopub.status.idle":"2023-04-25T20:19:32.112201Z","shell.execute_reply.started":"2023-04-25T20:19:32.100128Z","shell.execute_reply":"2023-04-25T20:19:32.110972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again categorise the numerical columns and categorical columns.","metadata":{}},{"cell_type":"markdown","source":"# 6. FEATURE GENERATION\n***","metadata":{}},{"cell_type":"code","source":"\ndf[\"NEW_1st*GrLiv\"] = df[\"1stFlrSF\"] * df[\"GrLivArea\"]\n\ndf[\"NEW_Garage*GrLiv\"] = (df[\"GarageArea\"] * df[\"GrLivArea\"])\n\n\n# Total Floor\ndf[\"NEW_TotalFlrSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"] # 32\n\n# Total Finished Basement Area\ndf[\"NEW_TotalBsmtFin\"] = df.BsmtFinSF1 + df.BsmtFinSF2 # 56\n\n# Porch Area\ndf[\"NEW_PorchArea\"] = df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch +  df.WoodDeckSF # 93\n\n# Total House Area\ndf[\"NEW_TotalHouseArea\"] = df.NEW_TotalFlrSF + df.TotalBsmtSF # 156\n\ndf[\"NEW_TotalSqFeet\"] = df.GrLivArea + df.TotalBsmtSF # 35\n\n\n# Lot Ratio\ndf[\"NEW_LotRatio\"] = df.GrLivArea / df.LotArea # 64\n\ndf[\"NEW_RatioArea\"] = df.NEW_TotalHouseArea / df.LotArea # 57\n\ndf[\"NEW_GarageLotRatio\"] = df.GarageArea / df.LotArea # 69\n\n# MasVnrArea\ndf[\"NEW_MasVnrRatio\"] = df.MasVnrArea / df.NEW_TotalHouseArea # 36\n\n# Dif Area\ndf[\"NEW_DifArea\"] = (df.LotArea - df[\"1stFlrSF\"] - df.GarageArea - df.NEW_PorchArea - df.WoodDeckSF) # 73\n\n\ndf[\"NEW_OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"] # 61\n\n\ndf[\"NEW_Restoration\"] = df.YearRemodAdd - df.YearBuilt # 31\n\ndf[\"NEW_HouseAge\"] = df.YrSold - df.YearBuilt # 73\n\ndf[\"NEW_RestorationAge\"] = df.YrSold - df.YearRemodAdd # 40\n\ndf[\"NEW_GarageAge\"] = df.GarageYrBlt - df.YearBuilt # 17\n\ndf[\"NEW_GarageRestorationAge\"] = np.abs(df.GarageYrBlt - df.YearRemodAdd) # 30\n\ndf[\"NEW_GarageSold\"] = df.YrSold - df.GarageYrBlt # 48\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:32.114281Z","iopub.execute_input":"2023-04-25T20:19:32.114784Z","iopub.status.idle":"2023-04-25T20:19:32.143923Z","shell.execute_reply.started":"2023-04-25T20:19:32.114733Z","shell.execute_reply":"2023-04-25T20:19:32.142840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncat_cols_3, num_cols_3, cat_but_car = grab_col_names(df)\nnum_cols_3 = [col for col in num_cols_3 if col not in [\"SalePrice\",\"Id\"]]\n\ndf[num_cols_3].isna().sum()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:32.145345Z","iopub.execute_input":"2023-04-25T20:19:32.145750Z","iopub.status.idle":"2023-04-25T20:19:32.196726Z","shell.execute_reply.started":"2023-04-25T20:19:32.145713Z","shell.execute_reply":"2023-04-25T20:19:32.195467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.1 Handling New Missing Values","metadata":{}},{"cell_type":"code","source":"df[cat_cols_3]=df[cat_cols_3].apply(lambda x: np.where(x.isna(),\"No\",x))\ndef quick_missing_imp(data, num_method=\"median\", cat_length=20, target=\"SalePrice\"):\n    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]  # Eksik değere sahip olan değişkenler listelenir\n\n    temp_target = data[target]\n\n    print(\"# BEFORE\")\n    print(data[variables_with_na].isnull().sum(), \"\\n\\n\")  # Uygulama öncesi değişkenlerin eksik değerlerinin sayısı\n\n    # değişken object ve sınıf sayısı cat_lengthe eşit veya altındaysa boş değerleri mode ile doldur\n    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)\n\n    # num_method mean ise tipi object olmayan değişkenlerin boş değerleri ortalama ile dolduruluyor\n    if num_method == \"mean\":\n        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n    # num_method median ise tipi object olmayan değişkenlerin boş değerleri ortalama ile dolduruluyor\n    elif num_method == \"median\":\n        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n\n    data[target] = temp_target\n\n    print(\"# AFTER \\n Imputation method is 'MODE' for categorical variables!\")\n    print(\" Imputation method is '\" + num_method.upper() + \"' for numeric variables! \\n\")\n    print(data[variables_with_na].isnull().sum(), \"\\n\\n\")\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:32.198105Z","iopub.execute_input":"2023-04-25T20:19:32.198452Z","iopub.status.idle":"2023-04-25T20:19:32.255364Z","shell.execute_reply.started":"2023-04-25T20:19:32.198419Z","shell.execute_reply":"2023-04-25T20:19:32.254374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf = quick_missing_imp(df,num_method=\"median\",cat_length=17)\n\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. TRANSFORMATION AND SCALING\n***","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>GENERAL INFO:</b> Scaling, Standardizing and Transformation are important steps of numeric feature engineering and they are being used to treat skewed features and rescale them for modelling.\n</div>\n","metadata":{}},{"cell_type":"code","source":"def plot_data(df,col_name):\n    import scipy.stats as stat\n    import pylab\n    plt.figure(figsize=(10,8))\n    plt.subplot(1,2,1)\n    sns.histplot(df[col_name],kde=True)\n    plt.subplot(1,2,2)\n    stat.probplot(df[col_name],dist=\"norm\", plot=pylab)\n    plt.show(block=True)\n\ndef log_transformation(df,col_name,plot=False):\n    temp_df = df.copy()\n    temp_df[col_name+\"_log\"]=np.log(temp_df[col_name]+1)\n    if plot:\n        plot_data(temp_df,col_name+\"_log\")\n    return skew(temp_df[col_name+\"_log\"])\n\ndef reciprocal_transformation(df,col_name,plot=False):\n    temp_df = df.copy()\n    temp_df[col_name+\"_reciprocal\"]=1/(temp_df[col_name])\n    if plot:\n        plot_data(temp_df,col_name+\"_reciprocal\")\n    return skew(temp_df[col_name+\"_reciprocal\"])\n\ndef square_root_transformation(df,col_name,plot=False):\n    temp_df = df.copy()\n    temp_df[col_name+\"_square\"]=temp_df[col_name]**(1/2)\n\n    if plot:\n        plot_data(temp_df,col_name+\"_square\")\n    return skew(temp_df[col_name+\"_square\"])\n\ndef zscore_transformation(df,col_name,plot=False):\n    from scipy.stats import zscore\n    temp_df = df.copy()\n    temp_df[col_name+\"_zscore\"]=zscore(temp_df[col_name])\n\n    if plot:\n        plot_data(temp_df,col_name+\"_zscore\")\n    return skew(temp_df[col_name+\"_zscore\"])\n\ndef quantile_transform(df,col_name,out_dist = \"uniform\",n_q=1000,plot=False):\n    \"\"\"\n    Parameters\n    ----------\n    df: pd.Dataframe()\n        Dataframe\n    col_name: string\n        Column name which you wantedd to apply this fuction\n    out_dist:{‘uniform’, ‘normal’}, default=’uniform’\n    Marginal distribution for the transformed data. The choices are ‘uniform’ (default) or ‘normal’.\n    n_q:n_quantiles int, default=1000 or n_samples\n\n    plot Bool, default=False\n        Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default)\n\n    Returns\n    -------\n    Compute the sample skewness of a data set.\n    \"\"\"\n    from sklearn.preprocessing import QuantileTransformer\n    temp_df=df.copy()\n    quantile = QuantileTransformer(output_distribution=out_dist,n_quantiles=n_q)\n    temp_df[col_name+\"_quant\"] = quantile.fit_transform(df[[col_name]].values)\n    if plot:\n        plot_data(temp_df,col_name+\"_quant\")\n    return skew(temp_df[col_name+\"_quant\"])\n\ntransform_list = [log_transformation,reciprocal_transformation,square_root_transformation,zscore_transformation,quantile_transform]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:32.404307Z","iopub.execute_input":"2023-04-25T20:19:32.404725Z","iopub.status.idle":"2023-04-25T20:19:32.421297Z","shell.execute_reply.started":"2023-04-25T20:19:32.404686Z","shell.execute_reply":"2023-04-25T20:19:32.420111Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in transform_list:\n    print(i)\n    for j in num_cols_3:\n\n        print(f\"{j} transform. Skew value:{i(df,j)}\")\n    print(\"*\"*40)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:32.423083Z","iopub.execute_input":"2023-04-25T20:19:32.423433Z","iopub.status.idle":"2023-04-25T20:19:33.445969Z","shell.execute_reply.started":"2023-04-25T20:19:32.423400Z","shell.execute_reply":"2023-04-25T20:19:33.444607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>RESULT: </b> Pls, use which one is proper your data. When i compared the skew values, quantile transformation is the smallest value.\nIt means the columns get closer to the normal distribution\n</div>","metadata":{}},{"cell_type":"code","source":"\ndef apply_quantile(df,col_name):\n    from sklearn.preprocessing import QuantileTransformer\n    quantile = QuantileTransformer(output_distribution=\"uniform\")\n    df[col_name] = quantile.fit_transform(df[[col_name]].values)\n    return skew(df[col_name])\n\nfor i in num_cols_3:\n    apply_quantile(df,i)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:33.454186Z","iopub.execute_input":"2023-04-25T20:19:33.454583Z","iopub.status.idle":"2023-04-25T20:19:34.028866Z","shell.execute_reply.started":"2023-04-25T20:19:33.454533Z","shell.execute_reply":"2023-04-25T20:19:34.027502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. ENCODING\n***","metadata":{}},{"cell_type":"markdown","source":"# 8.1 LABEL ENCODING","metadata":{}},{"cell_type":"code","source":"def label_encoder(dataframe, binary_col):\n    from sklearn.preprocessing import LabelEncoder\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\nbinary_cols = [col for col in df.columns if df[col].dtypes == \"O\" and len(df[col].unique()) == 2]\n\nfor col in binary_cols:\n    label_encoder(df, col)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:34.030884Z","iopub.execute_input":"2023-04-25T20:19:34.031354Z","iopub.status.idle":"2023-04-25T20:19:34.059344Z","shell.execute_reply.started":"2023-04-25T20:19:34.031304Z","shell.execute_reply":"2023-04-25T20:19:34.058338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8.2 ONE HOT ENCODING","metadata":{}},{"cell_type":"code","source":"\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndf = one_hot_encoder(df, cat_cols_3, drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:34.061007Z","iopub.execute_input":"2023-04-25T20:19:34.061787Z","iopub.status.idle":"2023-04-25T20:19:34.114968Z","shell.execute_reply.started":"2023-04-25T20:19:34.061740Z","shell.execute_reply":"2023-04-25T20:19:34.113743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Feature Selection, In this part, we ll select features by using tree-based feature selection method. Firstly, I will select a feature number by comparing LassoCV, RidgeCV & ElasticCV. Those models give the minumum number of feature numbers for ml. For example, in this project, The smallest number of feaure is selected by LassoCV. I will select this method. I will combine the advice of multiple, different, models to decide on which features are worth keeping. Firstly i selected LassoCV & i will choose tow models from RandomForestRegressor,LGBMRegressor,XGBRegressor, AdaBoostRegressor & GradientBoostingRegressor*","metadata":{}},{"cell_type":"code","source":"\n\ndef split_model (x,y,random_s= 42, test_s=0.3):\n\n    x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=random_s,test_size=test_s)\n\n    return x_train,x_test,y_train,y_test\n\n\ndef number_of_feature_selection(x,y,Lasso = False,Ridge =False, Elatic =False):\n    \"\"\"\n\n    Parameters\n    ----------\n    x: Features\n    y: Target feature\n    Lasso: LassoCV\n    Ridge: RidgeCV\n    Elatic: ElasticNetCV\n\n    Returns\n    -------\n    Model names and min requirements for best modelling.\n    And mask list of each of three model.\n\n    \"\"\"\n    x_train, x_test, y_train, y_test = split_model(x,y)\n\n    linear_CV = [LassoCV,RidgeCV,ElasticNetCV]\n    n_feature = []\n\n    mask_liste = []\n\n    if Lasso:\n        reg = LassoCV(cv=5).fit(X=x_train, y=y_train)\n        mask = reg.coef_ != 0\n        mask_liste.append(mask)\n        print(mask.sum())\n        liste = []\n        reduced_x = x.loc[:, mask].columns\n        liste.append(reduced_x)\n        n_feature.append(liste)\n    elif Ridge:\n        reg = RidgeCV(cv=5).fit(X=x_train, y=y_train)\n        mask = reg.coef_ != 0\n        mask_liste.append(mask)\n        print(mask.sum())\n        liste = []\n        reduced_x = x.loc[:, mask].columns\n        liste.append(reduced_x)\n        n_feature.append(liste)\n    elif Elatic:\n        reg = ElasticNetCV(cv=5).fit(X=x_train, y=y_train)\n        mask = reg.coef_ != 0\n        mask_liste.append(mask)\n        print(mask.sum())\n        liste = []\n        reduced_x = x.loc[:, mask].columns\n        liste.append(reduced_x)\n        n_feature.append(liste)\n\n    else:\n        for i in linear_CV:\n\n            print(i)\n            reg = i(cv=5).fit(X=x_train, y=y_train)\n            mask = reg.coef_ != 0\n            mask_liste.append(mask)\n            print(mask.sum())\n            liste = []\n            reduced_x = x.loc[:, mask].columns\n            liste.append(reduced_x)\n            n_feature.append(liste)\n\n    return n_feature,mask_liste\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:34.116945Z","iopub.execute_input":"2023-04-25T20:19:34.117405Z","iopub.status.idle":"2023-04-25T20:19:34.132814Z","shell.execute_reply.started":"2023-04-25T20:19:34.117355Z","shell.execute_reply":"2023-04-25T20:19:34.131470Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df[df['SalePrice'].notnull()]\ntest_df = df[df['SalePrice'].isnull()]\n\nx = train_df.drop([\"Id\",\"SalePrice\"],axis=1)\ny = train_df[\"SalePrice\"]\ny_ = train_df[[\"SalePrice\"]]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:34.134805Z","iopub.execute_input":"2023-04-25T20:19:34.135272Z","iopub.status.idle":"2023-04-25T20:19:34.152520Z","shell.execute_reply.started":"2023-04-25T20:19:34.135226Z","shell.execute_reply":"2023-04-25T20:19:34.151400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>RESULT: </b> By using LassoCV, RidgeCV & ElasticCV, we selected min feature requirements for model. Thus, i will select Lasso for combining moodels to select features which are worth keeping.\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"# 9. FEATURE SELECTION-2\n***","metadata":{}},{"cell_type":"code","source":"model_liste = [GradientBoostingRegressor(), AdaBoostRegressor(), XGBRegressor(),LGBMRegressor(),RandomForestRegressor()]\ndef feature_selection(x,y,model_liste,limitation=2,step_ = 20):\n    \"\"\"\n\n    Parameters\n    ----------\n    x: Features\n    y: Target feature\n    model_liste: list of model\n    limitation: In the function, 3 models used. If 2/3 of the models choose the features. It wll be kept.\n    step_: If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to remove at each iteration.\n\n    Returns\n    -------\n    DataFrame of three based model (combined models to select feature) with r2 and number of features.\n    Reduced list of each  three based model.\n    Dataframe of mask list\n    \"\"\"\n    liste,mask_liste = number_of_feature_selection(x,y,Lasso=True)\n\n    models = model_liste\n    final_result = []\n    reduced_x_list = []\n\n    j=0\n    while j < len(models):\n        for k in models[j:]:\n            if k != models[j]:\n                x_train, x_test, y_train, y_test = split_model(x, y)\n                print(\"Model:\", models[j], \",\", k)\n                reg_1 = RFE(estimator=models[j],\n                            n_features_to_select=len(liste[0][0]),\n                            step=step_,\n                            verbose=1)\n                reg_1.fit(x_train, y_train)\n                mask_1 = reg_1.support_\n\n                reg_2 = RFE(estimator=k,\n                            n_features_to_select=len(liste[0][0]),\n                            step=step_,\n                            verbose=1)\n                reg_2.fit(x_train, y_train)\n                mask_2 = reg_2.support_\n                mask_0 = mask_liste[0]\n\n                votes = np.sum([mask_0, mask_1, mask_2], axis=0)\n\n                masked = votes >= limitation\n\n                reduced_x = x.loc[:, masked]\n\n                x_train, x_test, y_train, y_test = split_model(reduced_x, y)\n\n                le = LinearRegression()\n                le.fit(x_train, y_train)\n                r_squared = le.score(x_test, y_test)\n                print(\n                    f'The model can explain {r_squared:.1%} of the variance in the test set using {len(le.coef_)} features.')\n                reduced_x_list.append([models[j],k,reduced_x])\n                final_result.append([models[j], k, r_squared, len(le.coef_)])\n\n        j += 1\n\n    df = pd.DataFrame(final_result,columns=[\"model_1\",\"model_2\",\"r2\",\"number_of_feature\"])\n    df_reduced_x =pd.DataFrame(reduced_x,columns=reduced_x.columns)\n\n    return df,reduced_x_list,df_reduced_x\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:19:34.154355Z","iopub.execute_input":"2023-04-25T20:19:34.154799Z","iopub.status.idle":"2023-04-25T20:19:34.170206Z","shell.execute_reply.started":"2023-04-25T20:19:34.154754Z","shell.execute_reply":"2023-04-25T20:19:34.168917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_model,reduced_x_list,df_reduced_x = feature_selection(x,y,model_liste,limitation=2, step_=20)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:21:46.320084Z","iopub.execute_input":"2023-04-25T20:21:46.321328Z","iopub.status.idle":"2023-04-25T20:26:00.401255Z","shell.execute_reply.started":"2023-04-25T20:21:46.321275Z","shell.execute_reply":"2023-04-25T20:26:00.399623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_model","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:26:54.382723Z","iopub.execute_input":"2023-04-25T20:26:54.383126Z","iopub.status.idle":"2023-04-25T20:26:54.415101Z","shell.execute_reply.started":"2023-04-25T20:26:54.383092Z","shell.execute_reply":"2023-04-25T20:26:54.413687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>RESULT: </b> Selectting features by using AdaBoostRegressor() & GradientBoostingRegressor(). Feature number is dropped from 254 to 60.\n</div>","metadata":{}},{"cell_type":"code","source":"feature_model,reduced_x_list,df_reduced_x = feature_selection(x,y,[RandomForestRegressor(), XGBRegressor()],limitation=2, step_=20)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:28:35.046176Z","iopub.execute_input":"2023-04-25T20:28:35.046621Z","iopub.status.idle":"2023-04-25T20:29:17.435232Z","shell.execute_reply.started":"2023-04-25T20:28:35.046578Z","shell.execute_reply":"2023-04-25T20:29:17.433931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9.1 MASK TEST & TRAIN DATA SET","metadata":{}},{"cell_type":"code","source":"mask = df_reduced_x.columns","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:29:40.998926Z","iopub.execute_input":"2023-04-25T20:29:40.999377Z","iopub.status.idle":"2023-04-25T20:29:41.005413Z","shell.execute_reply.started":"2023-04-25T20:29:40.999333Z","shell.execute_reply":"2023-04-25T20:29:41.004076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=test_df[mask]\ntrain_df = train_df[mask]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:29:45.976331Z","iopub.execute_input":"2023-04-25T20:29:45.977198Z","iopub.status.idle":"2023-04-25T20:29:45.984189Z","shell.execute_reply.started":"2023-04-25T20:29:45.977151Z","shell.execute_reply":"2023-04-25T20:29:45.982854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_df_wo_saleprice= pd.concat([train_df,test_df])\nreduced_df = pd.concat([reduced_df_wo_saleprice,y],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:29:47.503471Z","iopub.execute_input":"2023-04-25T20:29:47.504522Z","iopub.status.idle":"2023-04-25T20:29:47.513126Z","shell.execute_reply.started":"2023-04-25T20:29:47.504473Z","shell.execute_reply":"2023-04-25T20:29:47.511982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. DIMENSIONALITY REDUCTION\n***","metadata":{}},{"cell_type":"code","source":"train_df = reduced_df[reduced_df['SalePrice'].notnull()]\ntest_df = reduced_df[reduced_df['SalePrice'].isnull()]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:29:49.198290Z","iopub.execute_input":"2023-04-25T20:29:49.198725Z","iopub.status.idle":"2023-04-25T20:29:49.210781Z","shell.execute_reply.started":"2023-04-25T20:29:49.198685Z","shell.execute_reply":"2023-04-25T20:29:49.209333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_df['SalePrice']\ny_= train_df[['SalePrice']]\nx = train_df.drop([\"SalePrice\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T21:12:36.172613Z","iopub.execute_input":"2023-04-25T21:12:36.173404Z","iopub.status.idle":"2023-04-25T21:12:36.181424Z","shell.execute_reply.started":"2023-04-25T21:12:36.173355Z","shell.execute_reply":"2023-04-25T21:12:36.179972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:29:51.787797Z","iopub.execute_input":"2023-04-25T20:29:51.788335Z","iopub.status.idle":"2023-04-25T20:29:51.797858Z","shell.execute_reply.started":"2023-04-25T20:29:51.788284Z","shell.execute_reply":"2023-04-25T20:29:51.796576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10.1 PCA DECOMPOSITION","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = split_model(x,y)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:29:57.762587Z","iopub.execute_input":"2023-04-25T20:29:57.763377Z","iopub.status.idle":"2023-04-25T20:29:57.769767Z","shell.execute_reply.started":"2023-04-25T20:29:57.763326Z","shell.execute_reply":"2023-04-25T20:29:57.768892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=0.991)\nx_train_pca = pca.fit_transform(x_train)\nx_test_pca = pca.transform(x_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:29:59.728311Z","iopub.execute_input":"2023-04-25T20:29:59.729339Z","iopub.status.idle":"2023-04-25T20:29:59.753324Z","shell.execute_reply.started":"2023-04-25T20:29:59.729297Z","shell.execute_reply":"2023-04-25T20:29:59.751746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x_train_pca[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:30:01.279623Z","iopub.execute_input":"2023-04-25T20:30:01.280573Z","iopub.status.idle":"2023-04-25T20:30:01.287176Z","shell.execute_reply.started":"2023-04-25T20:30:01.280512Z","shell.execute_reply":"2023-04-25T20:30:01.286114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reduced features by PCA Decomposition. Feature number is reduced from 66 to 49.","metadata":{}},{"cell_type":"markdown","source":"# 11. MODELLING\n***","metadata":{}},{"cell_type":"code","source":"models = [('KNN', KNeighborsRegressor()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"AdaBoost\",AdaBoostRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:30:22.373340Z","iopub.execute_input":"2023-04-25T20:30:22.374289Z","iopub.status.idle":"2023-04-25T20:30:22.380601Z","shell.execute_reply.started":"2023-04-25T20:30:22.374238Z","shell.execute_reply":"2023-04-25T20:30:22.379016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, regressor in models:\n    RMSE = np.mean(np.sqrt(-cross_val_score(regressor, df_reduced_x, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"R2: {round(RMSE, 4)} ({name})\")","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:30:23.472685Z","iopub.execute_input":"2023-04-25T20:30:23.473097Z","iopub.status.idle":"2023-04-25T20:30:57.946649Z","shell.execute_reply.started":"2023-04-25T20:30:23.473060Z","shell.execute_reply":"2023-04-25T20:30:57.945521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, regressor in models:\n    r2 = np.mean(cross_val_score(regressor, df_reduced_x, y, cv=5, scoring=\"r2\"))\n    print(f\"R2: {round(r2, 4)} ({name})\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:30:57.948359Z","iopub.execute_input":"2023-04-25T20:30:57.948708Z","iopub.status.idle":"2023-04-25T20:31:32.915441Z","shell.execute_reply.started":"2023-04-25T20:30:57.948674Z","shell.execute_reply":"2023-04-25T20:31:32.914245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>RESULT: </b> Best model for this data set is that CatBoostRegression.\n</div>","metadata":{}},{"cell_type":"markdown","source":"# 12 .OPTIMIZATION\n***","metadata":{}},{"cell_type":"markdown","source":"# 12.1 Modelling with PCA","metadata":{}},{"cell_type":"code","source":"\ncatboost_params = {\"iterations\": [1000,2000],\n                   \"learning_rate\": [0.01,0.2]}\ncatboost_model = CatBoostRegressor(random_state=46)\n\ncatboost_gradient = GridSearchCV(catboost_model,\n                            catboost_params,\n                            cv=5,\n                            verbose=True,\n                            n_jobs=-1\n                            ).fit(x_train_pca,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:37:28.027893Z","iopub.execute_input":"2023-04-25T20:37:28.028281Z","iopub.status.idle":"2023-04-25T20:40:52.283495Z","shell.execute_reply.started":"2023-04-25T20:37:28.028246Z","shell.execute_reply":"2023-04-25T20:40:52.282012Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost_gradient.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:41:24.874451Z","iopub.execute_input":"2023-04-25T20:41:24.875769Z","iopub.status.idle":"2023-04-25T20:41:24.883650Z","shell.execute_reply.started":"2023-04-25T20:41:24.875720Z","shell.execute_reply":"2023-04-25T20:41:24.882208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_model_pca = CatBoostRegressor(**catboost_gradient.best_params_).fit(x_train_pca,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:41:26.964640Z","iopub.execute_input":"2023-04-25T20:41:26.965402Z","iopub.status.idle":"2023-04-25T20:41:39.775242Z","shell.execute_reply.started":"2023-04-25T20:41:26.965356Z","shell.execute_reply":"2023-04-25T20:41:39.774174Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_rmse_pca = np.mean(np.sqrt(-cross_val_score(cb_model_pca, x_train_pca, y_train, cv=5, scoring=\"neg_mean_squared_error\")))","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:41:41.293472Z","iopub.execute_input":"2023-04-25T20:41:41.294679Z","iopub.status.idle":"2023-04-25T20:42:45.807023Z","shell.execute_reply.started":"2023-04-25T20:41:41.294618Z","shell.execute_reply":"2023-04-25T20:42:45.805650Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_rmse_pca","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:43:07.427650Z","iopub.execute_input":"2023-04-25T20:43:07.428414Z","iopub.status.idle":"2023-04-25T20:43:07.435626Z","shell.execute_reply.started":"2023-04-25T20:43:07.428369Z","shell.execute_reply":"2023-04-25T20:43:07.434462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12.2 Modelling without PCA","metadata":{}},{"cell_type":"code","source":"cb_model_wo_pca = CatBoostRegressor(**catboost_gradient.best_params_).fit(x_train,y_train)\n\ncb_rmse_wo_pca = np.mean(np.sqrt(-cross_val_score(cb_model_wo_pca, x_train_pca, y_train, cv=5, scoring=\"neg_mean_squared_error\")))","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:43:19.185925Z","iopub.execute_input":"2023-04-25T20:43:19.186349Z","iopub.status.idle":"2023-04-25T20:44:30.833040Z","shell.execute_reply.started":"2023-04-25T20:43:19.186308Z","shell.execute_reply":"2023-04-25T20:44:30.831946Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_rmse_wo_pca ","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:44:39.601186Z","iopub.execute_input":"2023-04-25T20:44:39.601824Z","iopub.status.idle":"2023-04-25T20:44:39.607567Z","shell.execute_reply.started":"2023-04-25T20:44:39.601785Z","shell.execute_reply":"2023-04-25T20:44:39.606688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>RESULT: </b> When comparing catboost regression with pca decomposition and without it, their rmse values are almost the same. Thus, it will be good preferance to work with reduced by PCA.\n</div>","metadata":{}},{"cell_type":"markdown","source":"# 13. FEATURE IMPORTANCE\n***","metadata":{}},{"cell_type":"code","source":"colu = x_train.columns[:len(x_train_pca[0])]\ncolu","metadata":{"execution":{"iopub.status.busy":"2023-04-25T21:12:55.037407Z","iopub.execute_input":"2023-04-25T21:12:55.038718Z","iopub.status.idle":"2023-04-25T21:12:55.046746Z","shell.execute_reply.started":"2023-04-25T21:12:55.038650Z","shell.execute_reply":"2023-04-25T21:12:55.045397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp = pd.DataFrame({\"Value\": cb_model_pca.feature_importances_, \"Feature\": colu})\nplt.figure(figsize=(10, 10))\nsns.set(font_scale=1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:10])\nplt.title(\"Features\")\nplt.tight_layout()\nplt.show(block=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T21:12:58.399296Z","iopub.execute_input":"2023-04-25T21:12:58.399704Z","iopub.status.idle":"2023-04-25T21:12:58.793015Z","shell.execute_reply.started":"2023-04-25T21:12:58.399669Z","shell.execute_reply":"2023-04-25T21:12:58.792096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}